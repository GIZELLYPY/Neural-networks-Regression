{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLPRegressor do módulo sklearn.neural_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Por que regressão com MLP?\n",
    "\n",
    "De maneira mais geral, porque alguns problemas de aprendizado profundo são problemas de regressão.\n",
    "\n",
    "E, assim como na classificação, o uso de perceptrons de várias camadas é um bom ponto de partida para aprender sobre as arquiteturas mais complexas usadas para a regressão no aprendizado profundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "\n",
    "(X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize regressor perceptron de várias camadas importando a classe MLPRegressor do módulo sklearn.neural_network e, em seguida, crie o objeto MLPRegressor. \n",
    "\n",
    "Ao criar o objeto, definimos o número de camadas e unidades ocultas em cada camada oculta. Usando o mesmo parâmetro hidden_layer_sizes.\n",
    "\n",
    "E observe que primeiro aplicamos o <b>MinMaxScaler</b> para pré-processar os recursos de entrada.\n",
    "\n",
    "Aqui, combinaremos uma rede mais complexa, usando 2 camadas ocultas com 100 unidades cada. Com uma configuração de regularização mais alta de alfa em 5.0 e usando o solucionador lgbfs novamente.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> Novamente, como na classificação, o efeito de aumentar a quantidade de regularização de L2, aumentando o alfa é restringir a regressão para usar modelos cada vez mais simples, com pesos cada vez menores.\n",
    "\n",
    "\n",
    "A principal maneira de controlar a complexidade do modelo para o MLP é controlar o tamanho e a estrutura da unidade oculta. Usando o parâmetro hidden_layers_sizes que controla o número de camadas ocultas e o número de unidades dentro de cada camada.\n",
    "\n",
    "Alpha controla a quantidade de regularização que ajuda a restringir a complexidade do modelo, restringindo a magnitude dos pesos do modelo.\n",
    "\n",
    "Finalmente, você pode experimentar pelo menos três opções diferentes para a função de ativação não linear, usando o parâmetro de ativação.\n",
    "\n",
    "Anteriormente, vimos o parâmetro solver, para especificar o algoritmo que aprende os pesos da rede.\n",
    "\n",
    "<b>lbfgs</b> solver é o algoritmo que realmente faz o trabalho numérico de encontrar os pesos ideais. E uma maneira intuitiva de visualizar esse processo. É que todos os algoritmos do solucionador precisam fazer uma espécie de escalada em uma paisagem muito acidentada, com muitos mínimos locais. Onde cada mínimo local corresponde a um conjunto ideal localmente de pesos. Ou seja, uma escolha de configuração de peso melhor do que qualquer opção próxima de pesos. Assim, em toda essa paisagem de mínimos locais muito esburacados. Alguns terão pontuações mais altas de validação nos dados do teste e outros terão menores. Então, dependendo da inicialização aleatória inicial dos pesos. E a natureza da trajetória no caminho de busca que um solucionador percorre nessa paisagem esburacada. O <b>lbfgs</b> solver pode terminar em diferentes mínimos locais, que podem ter diferentes pontuações de validação. O solver padrão, <b>adam</b>, tende a ser eficiente e eficaz em grandes conjuntos de dados, com milhares de exemplos de treinamento. Para conjuntos de dados pequenos, como muitos dos que usamos nesses exemplos, o solucionador de <b>lbfgs</b> tende a ser mais rápido e a encontrar pesos mais eficazes. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast cancer dataset\n",
      "Acurácia training set: 0.98\n",
      "Acurácia test set: 0.97\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# Muitos algoritmos de aprendizado de máquina têm melhor desempenho quando as variáveis de entrada \n",
    "#numéricas são escaladas para um intervalo padrão. Isso inclui algoritmos que usam uma soma \n",
    "#ponderada da entrada, como regressão linear, e algoritmos que usam medidas de distância, \n",
    "#como k-vizinhos mais próximos. As duas técnicas mais populares para dimensionar dados numéricos \n",
    "#antes da modelagem são normalização e padronização. A normalização dimensiona cada variável de \n",
    "#entrada separadamente para o intervalo 0-1, que é o intervalo para valores de ponto flutuante onde\n",
    "#temos mais precisão. A padronização dimensiona cada variável de entrada separadamente subtraindo \n",
    "#a média (chamada de centralização) e dividindo \n",
    "#pelo desvio padrão para mudar a distribuição para ter uma média zero e um desvio padrão um.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes = [100, 100], alpha = 5.0,\n",
    "                   random_state = 0, solver='lbfgs').fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Breast cancer dataset')\n",
    "print('Acurácia training set: {:.2f}'\n",
    "     .format(clf.score(X_train_scaled, y_train)))\n",
    "print('Acurácia test set: {:.2f}'\n",
    "     .format(clf.score(X_test_scaled, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
